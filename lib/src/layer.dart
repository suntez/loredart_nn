import 'dart:math' as math;
import 'package:loredart_nn/loredart_nn.dart';

import 'math_utils.dart';

/// Base super-class for any layer of [NeuralNetwork]
abstract class Layer {
  /// Number of rows in output [Matrix.column] after activation (or applying) of the [Layer] over input data
  late int units;

  /// The name of the layer
  String? name;

  /// Identify if initialization was called
  bool wasInitialized = false;

  /// Identify if [this] is trainable
  bool trainable = true;

  /// [Matrix] of the `weights` of the [Layer]
  Matrix? w;

  /// [Matrix.column] of the `biases` of the [Layer]
  Matrix? b;

  /// Defines if train biases or keep them as zero vector
  final bool useBiases;

  /// Input data buffer used in the learning proccess
  Matrix? inputDataBuffer;

  /// Derivatives of the activation function used in the learning process
  List<Matrix>? activatedDerivativeBuffer;

  Layer(this.units, this.name, {this.trainable = true, this.useBiases = true})
      : assert(units >= 0);

  /// Initialization of [Layer]'s parametrs
  /// should be called before calling ([act()]) the Layer
  void init([dynamic parametr]);

  /// Apply [Layer]'s logic to the inputs
  Matrix act(dynamic inputs, {bool train = false});

  /// After-training method, typically clear buffered data from the training process
  void clear() {
    inputDataBuffer = null;
    activatedDerivativeBuffer = null;
  }
}

/// Input [Layer] of Neural Network
/// Transform input [List<double>] of data into [Matrix]
/// This [Layer] is generated by [NeuralNetwork] and shouldn't be included into the list of model's [Layer]s
class Input extends Layer {
  Input(int units, {String? name}) : super(units, name, trainable: false);

  @override
  void init([dynamic parametr]) {}

  @override
  Matrix act(dynamic inputs, {bool train = false}) {
    return Matrix.fromLists(inputs).T;
  }

  @override
  String toString() {
    return "${name ?? 'input_layer'} -> Input($units)";
  }
}

/// Simple fully-connected [Layer]
///
/// Execute regular logic of dense-connected layer over given input data [inputs]:
///
/// `activation.function(weights*inputs+biases)`
///
/// Example:
/// ```dart
/// // without NeuralNetwork
/// final inputs = Matrix.column([-1, 1, 2]); // inputs len == 3
/// final dense = Dense(1, activation: Activation.elu());
/// dense.init(3);
/// final output = dense.act(inputs);
/// print(output); // output is matrix 1⨯1
///
/// // within NeuralNetwork
/// final nn = NeuralNetwork(10, [
///   Dense(32, activation: Activation.swish()),
///   Dense(1, activation: Activation.linear(), useBiases: false)
/// ], loss: Loss.mse());
/// ```
class Dense extends Layer {
  /// [Activation] function of the [Dense] layer
  late Activation activation;

  /// input vector lenght
  int? _prevDim;

  Dense(int units, {Activation? activation, String? name, bool? useBiases})
      : assert(units > 0),
        super(units, name, useBiases: useBiases ?? true) {
    this.activation = activation ?? Activation.linear();
  }

  /// Set initial values to weights and biases
  ///
  /// [dims] parametr is input vector length
  ///
  /// For weights [w] is used `Matrix(m, n, seed)` constructor with He-normal initialization
  ///
  /// For biases [b] is used `Matrix.zeros(n: n, m: 1)` constructor
  @override
  void init([dynamic dims]) {
    _prevDim = (dims as int);
    w = Matrix(n: units, m: _prevDim!, seed: 3);
    b = Matrix.zero(n: units, m: 1);
    wasInitialized = true;
  }

  /// Perform logic of [Dense] layer
  ///
  /// Return `activation.function(w*inputs+b)`
  ///
  /// Parameter [inputs] is expected to have dims [(batchSize, prevNumberOfUnits)]
  @override
  Matrix act(dynamic inputs, {bool train = false}) {
    if (!wasInitialized) {
      throw Exception(
        'Not initialized Layer.\nCall init() before act()');
    }
    final data = MatrixOperation.addVectorToEachColumn(w! * (inputs as Matrix), b!);
    if (train) {
      inputDataBuffer = inputs;
      activatedDerivativeBuffer = activation.dfunction(data);
    }
    return activation.function(data);
  }

  @override
  String toString() =>
      "${name ?? 'dense_layer'} -> Dense($units, activation: $activation)";
}

/// Types of normalization
enum NormalizationType { minMax, zScore }

/// Data normalization [Layer].
/// Normalize each record in batch independently.
/// 
/// Can be seen as column-wise normalization.
///
/// For now supports two different normalization types:
///
/// - [NormalizationType.minmax] -> `(data - min)/(max - min)`
/// - [NormalizationType.zScore] -> `(data-mean)/std`
///
/// [Layer] acts similar in [fit], [evaluate] and [predict] modes
///
/// Example:
/// ```dart
/// final data = Matrix.column([-2, -1, 0, 1, 2]);
/// final minmax = LayerNormalization(type: NormalizationType.minMax);
/// final zScore = LayerNormalization(type: NormalizationType.zScore);
/// minmax.init(5);
/// zScore.init(5);
///
/// // min-max normilized
/// print(minmax.act(data).flattenRow()); // transpose result
/// // Output: matrix 1⨯5 [[0.0, 0.25, 0.5, 0.75, 1.0]]
///
/// // z-score normilized
/// print(zScore.act(data).flattenRow()); // transpose result
/// //Output: matrix 1⨯5 [[-1.4142135, -0.707106781, 0.0, 0.707106781, 1.4142135]]
/// ```
/// 
/// For `batchSize` > 1 in NeuralNetwork these [Layer] applies normalization logic for each column of bacth matrix
/// 
/// Example:
/// ```dart
/// final batch = Matrix.fromLists(
///     [[-4, -1, 0, 2, 3], // first data record
///      [-4, -2, 1, 2, 3]] // second data record
///   ).T; // transpose so records are represented as columns
/// 
///   print(batch);
///   // Output:
///   // matrix 5⨯2
///   // [[-4.0, -4.0]
///   // [-1.0, -2.0]
///   // [0.0, 1.0]
///   // [2.0, 2.0]
///   // [3.0, 3.0]]
///
/// 
///   final minmax = LayerNormalization(type: NormalizationType.minMax);
///   minmax.init(5);
/// 
///   var result = minmax.act(batch);
///   print(result);
///   // Output:
///   // matrix 5⨯2
///   // [[0.0, 0.0]
///   // [0.42857142857142855, 0.2857142857142857]
///   // [0.5714285714285714, 0.7142857142857142]
///   // [0.8571428571428571, 0.8571428571428571]
///   // [1.0, 1.0]]
/// ```
class LayerNormalization extends Layer {
  late NormalizationType type;
  LayerNormalization({this.type = NormalizationType.minMax, String? name})
      : super(0, name, trainable: false);

  /// Min-max normalization od [data]
  Matrix _minmax(Matrix data) {
    Matrix result = Matrix.zero(n: 0, m: 0);
    for (int i = 0; i < data.m; i += 1) {
      Matrix tempColumn = data.getColumn(i);
      final r = range(tempColumn);
      if (r[0] == r[1]) {
        tempColumn.scale(1 / r[0]);
      }
      else {
        tempColumn
          ..addScalar(-r[0])
          ..scale(1 / (r[1] - r[0]));
      }
      if (i == 0) {
        result = tempColumn;
      }
      else {
        result = MatrixOperation.columnBind(result, tempColumn);
      }
    }
    return result;
  }

  /// zScore normalization of [data]
  Matrix _zScore(Matrix data) {
    final mean = data.reduceMeanByAxis(1);
    // return data.addedScalar(-data.reduceMean()).scaled(1 / sd);
    Matrix result = Matrix.zero(n: 0, m: 0);
    for (int i = 0; i < data.m; i += 1) {
      Matrix tempColumn = data.getColumn(i);
      final sd = math.sqrt(tempColumn.apply((x) => math.pow(x - mean[0][i], 2).toDouble()).reduceMean());
      if (i == 0) {
        result = tempColumn.addedScalar(-mean[0][i]).scaled(sd);
      }
      else {
        result = MatrixOperation.columnBind(result, tempColumn.addedScalar(-mean[0][i]).scaled(sd));
      }
    }
    return result;
  }

  @override
  void init([parametr]) {
    units = (parametr as int);
  }

  @override
  Matrix act(inputs, {bool train = false}) {
    if (type == NormalizationType.zScore) {
      return _zScore(inputs as Matrix);
    } else {
      return _minmax(inputs as Matrix);
    }
  }

  @override
  String toString() => "${name ?? 'norm_layer'} -> Normalization()";
}
